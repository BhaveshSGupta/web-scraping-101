# Web Scraping 101

[![Join the chat at https://gitter.im/information-retrieval-101/Lobby](https://badges.gitter.im/information-retrieval-101/Lobby.svg)](https://gitter.im/information-retrieval-101/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Web Scraping (aka web mining) is the process of collecting information from a network resource. This document is a (WIP) introduction to differenct aspects and techniques involved in an web scraping system.

Most of this text will be language-agnostic, but code snippets will probably break that rule.

## The Process
You can crudely break down any web scraping process into 3 steps:

* acquiring or requesting a network resource
* parsing the payload and extracting primitives
* transformation and storage

There is also a 4th step that's concerned with finding _what_ to reterieve, but we'll come to that later.

systematic resources are generally easier to reterieve compared to non-systematic resources. What are systematic resources you ask? systematic resources are network resources that have a _pattern_ to them. One good example of a systematic resource is a infinitely scrolling webpage. You know that as long as you keep scrolling down you'll keep getting more content loaded, therefore it has a repeatable behaviour that you can rely on.

let's now walk through the web scraping process, step by step

### Step 1: acquiring or requesting a network resource
This should be self-explainatory. If you want to scrape data from a news website, you begin by first loading the contents of that webpage.

### Step 2: parsing the payload and extracting primitives
Once you have loaded a resource, the next logical step is to parse the resource data. Most of the time you'll be extracting data from a structured interchange langauge (XML, HTML, JSON et al). These representations make it exceedingly easy to extract and query data from them. Scraping data from non-structured text requires some arcane invocations that are beyond the scope of this text. Most of the time you'll load the interchange language into a data structure, and then extract `primitive` values. What are primitives you ask? They're simply values of interest. The reason they're called primitives is because they may go transformation down the line, and may end up being consumed (deleted/removed) in the process.

### Step 3: transformation and storage
This stage involves making changes to the extracted data so it fits your use case. It can range from simple pipelines that save some extracted data as is, to all sorts of sequential or concurrent transformations.

have some pseudo code:
```
resource = request('/resource/url')
data = parse(resource).extract('data')
save(data)
```

## An Example
```python
{{import(examples/python/hn_simple.py)}}
```

The aforementioned example depicts a simple scraper. Notice that this is just basic scraping, not crawling. What's the difference? A scraper's only job is to extract information from a payload, while the crawler finds the links to follow and then actually fetch them. Notice how our example only fetches the headings from the first page on hacker news. We're going to extend our example to introduce crawling, where our program will scrape data from more than one page. 

```python
{{import(examples/python/hn_crawl.py)}}
```
For brevity, I've kept it simple. But a crawler has to do a lot of jobs, like keep track of pages visited, checking for(and removing) circular references, reattempting any urls that fail etc. Okay, let's clean up our code and introduce some nice abstractions.

```python
{{import(examples/python/hn_refactored.py)}}
```
ahh there. all neat and tidy. Let's break it down:
* The Crawler is incharge of fetching data
* The Spiders extract the data you want
* The Pipeline does stuff with the said data.

The items in the pipeline are called in order in which the pipeline components were registered. The output of one component is piped into the input of the next. We will now further improve this example to fetch more data and store it in a JSON file.

But before we begin, lets crystalize our goals. we want:
* the name of the post
* It's URL
* link to comments
* user who posted it
* the time when it was scraped (collected)

Now that we have decided on our objectives, lets begin.

```python
{{import(examples/python/hn.py)}}
```

As you can see that this (final) crawler builds upon the previous, simpler version(s). We're now extracting some useful information and then sending it down the pipeline to be processed in a sequential fashion. Listings and paginated pages make for excellent and easy scrape. 

